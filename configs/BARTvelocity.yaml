train:
  num_epochs: 5
  accum_iter: 10
  batch_size: 8
  base_lr: 5e-6
  warmup: 4000
  finetune: True

pretrained_checkpoint: midi-bart-2023-12-26-19-05.pt
model_name: BART
dataset_name: 'roszcz/maestro-v1-sustain'
target: velocity
seed: 26

overfit: False

tokens_per_note: "multiple"
time_quantization_method: start
dataset:
  sequence_duration: 5
  sequence_step: 2

  quantization:
    start: 20
    duration: 3
    velocity: 3

device: "cuda:0"

log: True
log_frequency: 10
run_name: midi-bart-${now:%Y-%m-%d-%H-%M}
project: "midi-bart"

pre_defined_model: null

model:
  encoder_layers: 6
  encoder_ffn_dim: 2048
  encoder_attention_heads: 8
  decoder_layers: 6
  decoder_ffn_dim: 2048
  decoder_attention_heads: 8
  d_model: 512
